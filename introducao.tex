\section{Introdução}

O estudo da dinâmica das populações é frutífero para a matemática, como sabemos do estudo das equações diferenciais de Verhulst ou dos modelos clássicos de predador-presa, como o de Lotka-Volterra. Nesse trabalho, apresentamos uma teoria que também se originou no estudo da dinâmica populacional, mas que se baseia principalmente na modelagem da reprodução como sendo um processo estocástico: A teoria das Cadeias de Ramificação.

\vspace{0.2cm}

A ideia de utilizar um processo estocástico para estudar o crescimento populacional tem origem nos trabalhos de Francis Galton\footnote{Curiosidade: era também primo de Charles Darwin e foi o idealizador da eugenia} e de Henry William Watson, que estavam interessados na possível extinção de sobrenomes de aristocratas no Reino Unido. Em vista disso, a teoria das cadeias de ramificação carrega muitas nomenclaturas inspiradas no momento inicial de seu desenvolvimento, como veremos.

\vspace{0.2cm}

Uma descrição simples para uma cadeia de ramificação é a seguinte: um indivíduo de uma certa espécie se reproduz (e morre logo em seguida) de maneira assexuada, com o vetor $\textbf{p} = (p_1,p_2 \dots,p_k,\dots)$ sendo a distribuição de probabilidade para o tamanho de sua prole. Queremos modelar a quantidade de indivíduos $Z_n$ na enésima geração, considerando que todos indivíduos de uma mesma geração se reproduzem de maneira independente de acordo com \textbf{p} e que começamos em $Z_0 = 1$.

\vspace{0.2cm}

Estabelecido essa concepção do modelo, surgem as seguintes perguntas acerca de seu comportamento:
\begin{itemize}
	\item Qual o número esperado de filhos de cada indivíduo?
	\item Qual a esperança e a variância de $Z_n$?;
	\item É possível achar a distribuição de $Z_n$?;
	\item É possível achar a probabilidade de extinção da cadeia? (ou seja, a probabilidade de que haja um instante $N$ no qual $Z_N = 0$);
	\item É possível achar as condições para eventual extinção da cadeia?,
	\item Se é certa a extinção, podemos achar a distribuição do tempo até ela?
\end{itemize}

Planejamos, nas seguintes seções, desenvolver a teoria necessária para tentar responder as perguntas acima. Entretanto, vamos tentar aplicar os nossos conhecimentos já estabelecidos de probabilidade em um exemplo motivador básico. 

\subsection{Exemplo Motivador}

\begin{exemplo}{}{}
Tome $\textbf{p} \sim k \cdot \text{Bernoulli}(p)$, ou seja, cada indivíduo dessa espécie tem $k$ filhos com probabilidade $p$ ou não tem nenhum filho com probabilidade $1-p$.
\end{exemplo}

No exemplo 1.1.1, algumas das perguntas feitas inicialmente são relativamente fáceis de responder:

\begin{itemize}
	\item O número esperado de filhos é $k\mathbb{E}(\textbf{p}) = k \cdot p$;
	\item A distribuição condicional de $Z_n$ dado $Z_{n-1}$ é $k$ vezes uma binomial de parâmetros $Z_{n-1}, \, p$, pois enésima geração é composta exatamente pela quantidade de filhos que cada um dos indivíduos da geração anterior teve, e a soma de Bernoullis resulta numa binomial. Esse é o melhor que conseguimos chegar com nossa ferramentas atuais, visto que a distribuição não condicional já é muito complicada de se descrever;
	\item Pelo dito acima, temos que $\mathbb{E}(Z_n) = kp\mathbb{E}(Z_{n-1})$, logo, resolvendo a recorrência com a condição de contorno de que $\mathbb{E}(Z_1) = kp$, concluímos que $\mathbb{E}(Z_n) = (kp)^n$. Usando a lei da variância total, obtemos:

		\[Var(Z_n) = \mathbb{E}(Var(Z_n | Z_{n-1})) + Var(\mathbb{E}(Z_n | Z_{n-1}))\]
		\[Var(Z_n) =  \mathbb{E}(Z_{n-1}k^2p(1-p)) +k^2p^2Var(Z_{n-1})\]
		\[Var(Z_n) = k^2p(1-p)(kp)^{n-1} + k^2p^2Var(Z_n-1)\]

Usando a condição de contorno $Var(Z_1) = k^2p^2(1-p)^2$, obtemos que \[Var(Z_n) = k^{n+1}p^n(1-p)(1+(kp)+(kp)^2+ \dots + (kp)^{n-1})\]
\end{itemize}

\vspace{0.2cm}

Iremos abordar de maneira mais completa a probabilidade de extinção em seções posteriores, mas o modelo do exemplo 1.1.1 já nos leva a interessantes reflexões: a extinção ou não dos indivíduos modelo parece depender apenas da distribuição da prole; a esperança e a variância de $Z_n$ parecem ter o mesmo comportamento assintótico, isto é, ambas convergem ou divergem quando o $n \to \infty$; combinando as duas afirmações anteriores com a desigualdade de Markov, obtemos que:

\[\mathbb{P}(Z_n \geq 1) \leq \mathbb{E}(Z_n) = (kp)^n\]

Ou seja, apesar da média de $Z_n$  não nos dá informação muito útil quando ela é maior do que 1, mas parece nos dizer que $Z_n$ converge em probabilidade para 0 quando é estritamente menor do que 1.

Abaixo, estão os resultados da distribuição de $Z_{20}$ advindos de simulações implementadas no arquivo \verb|bernoulli.py|. 





